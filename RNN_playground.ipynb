{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhQEGP28RBSOKLrTOqNk8j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmadhyastha/.config/blob/master/RNN_playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71KUBvwzmqce"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load text data\n",
        "\n",
        "#txt_data = \"this is the NLP course at City, University of London \"\n",
        "txt_data = \"this NLP module is a module at a  \"\n",
        "# txt_data = open('input.txt', 'r').read() # test external files\n",
        "\n",
        "chars = list(set(txt_data)) # split and remove duplicate characters. convert to list.\n",
        "\n",
        "num_chars = len(chars) # the number of unique characters\n",
        "txt_data_size = len(txt_data)\n",
        "\n",
        "print(\"unique characters : \", num_chars) # You can see the number of unique characters in your input data.\n",
        "print(\"txt_data_size : \", txt_data_size)\n"
      ],
      "metadata": {
        "id": "N1NF-jjVmzXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encode\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(int_to_char)\n",
        "print(\"----------------------------------------------------\")\n",
        "# integer encode input data\n",
        "integer_encoded = [char_to_int[i] for i in txt_data] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
        "print(integer_encoded)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"data length : \", len(integer_encoded))"
      ],
      "metadata": {
        "id": "TlIwhbB3rCJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Not actually used.\n",
        "\n",
        "onehot_encoded = []\n",
        "\n",
        "for ix in integer_encoded: # ix is an index mapped to a unique character.\n",
        "    letter = [0 for _ in range(len(chars))] # A list len is equal to the number of unique characters and whose elements are all zero.\n",
        "    letter[ix] = 1 # 'letter' is a one-hot vector.\n",
        "    onehot_encoded.append(letter) # Add a 1d list(a vector for one character).\n",
        "onehot_encoded = np.array(onehot_encoded) # list to np-array\n",
        "\n",
        "print(onehot_encoded.shape)     #  = (len(data),len(chars))\n",
        "print(onehot_encoded)\n",
        "\n",
        "# invert encoding\n",
        "inverted = int_to_char[np.argmax(onehot_encoded[0])] # \"argmax\" returns the index of the largest value.\n",
        "print(inverted)\n"
      ],
      "metadata": {
        "id": "spOQTrelrENa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "\n",
        "iteration = 5000\n",
        "sequence_length = 10\n",
        "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
        "hidden_size = 100  # size of hidden layer of neurons.\n",
        "learning_rate = 1e-1\n",
        "\n",
        "\n",
        "# model parameters\n",
        "\n",
        "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden.\n",
        "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
        "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
        "\n",
        "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
        "b_y = np.zeros((num_chars, 1)) # output bias\n",
        "\n",
        "h_prev = np.zeros((hidden_size,1)) # h_(t-1)\n"
      ],
      "metadata": {
        "id": "3_RX5J9VrGF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forwardprop(inputs, targets, h_prev):\n",
        "\n",
        "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
        "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
        "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
        "    loss = 0 # loss initialization\n",
        "\n",
        "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).\n",
        "\n",
        "        xs[t] = np.zeros((num_chars,1))\n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state.\n",
        "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars.\n",
        "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
        "\n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
        "\n",
        "#         y_class = np.zeros((num_chars, 1))\n",
        "#         y_class[targets[t]] =1\n",
        "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)\n",
        "\n",
        "    return loss, ps, hs, xs"
      ],
      "metadata": {
        "id": "AnMHZ3ZHrKsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def backprop(ps, inputs, hs, xs):\n",
        "\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
        "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
        "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1)\n",
        "\n",
        "    # reversed\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
        "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy\n",
        "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h.\n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(W_hh.T, dhraw)\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.\n",
        "\n",
        "    return dWxh, dWhh, dWhy, dbh, dby\n",
        "\n"
      ],
      "metadata": {
        "id": "W_7HqyktrLuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_pointer = 0\n",
        "\n",
        "# memory variables for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
        "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
        "\n",
        "\n",
        "for i in range(iteration):\n",
        "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    data_pointer = 0 # go from start of data\n",
        "\n",
        "    for b in range(batch_size):\n",
        "\n",
        "        inputs = [char_to_int[ch] for ch in txt_data[data_pointer:data_pointer+sequence_length]]\n",
        "        targets = [char_to_int[ch] for ch in txt_data[data_pointer+1:data_pointer+sequence_length+1]] # t+1\n",
        "\n",
        "        if (data_pointer+sequence_length+1 >= len(txt_data) and b == batch_size-1): # processing of the last part of the input data.\n",
        "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
        "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
        "\n",
        "\n",
        "        # forward\n",
        "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
        "#         print(loss)\n",
        "\n",
        "        # backward\n",
        "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs)\n",
        "\n",
        "\n",
        "    # perform parameter update with Adagrad\n",
        "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y],\n",
        "                                    [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "            mem += dparam * dparam # elementwise\n",
        "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "        data_pointer += sequence_length # move data pointer\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
      ],
      "metadata": {
        "id": "Rsn77sH7rSz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test_char, length):\n",
        "    x = np.zeros((num_chars, 1))\n",
        "    x[char_to_int[test_char]] = 1\n",
        "    ixes = []\n",
        "    h = np.zeros((hidden_size,1))\n",
        "\n",
        "    for t in range(length):\n",
        "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h)\n",
        "        y = np.dot(W_hy, h) + b_y\n",
        "        p = np.exp(y) / np.sum(np.exp(y))\n",
        "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
        "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
        "        x = np.zeros((num_chars, 1)) # init\n",
        "        x[ix] = 1\n",
        "        ixes.append(ix) # list\n",
        "    txt = ''.join(int_to_char[i] for i in ixes)\n",
        "    print ('----\\n %s \\n----' % (txt, ))"
      ],
      "metadata": {
        "id": "X3HKb-TZrTIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict('a',30) # (char, len of output)"
      ],
      "metadata": {
        "id": "LXNoF4TksR_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "3XNhJpROtv94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, input_size)\n",
        "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_seq, hidden_state):\n",
        "        embedding = self.embedding(input_seq)\n",
        "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
        "        output = self.decoder(output)\n",
        "        return output, (hidden_state[0].detach(), hidden_state[1].detach())\n",
        "\n",
        "def train():\n",
        "    ########### Hyperparameters ###########\n",
        "    hidden_size = 512   # size of hidden state\n",
        "    seq_len = 100       # length of LSTM sequence\n",
        "    num_layers = 3      # num of layers in LSTM layer stack\n",
        "    lr = 0.002          # learning rate\n",
        "    epochs = 100        # max number of epochs\n",
        "    op_seq_len = 200    # total num of characters in output test sequence\n",
        "    load_chk = False    # load weights from save_path directory to continue training\n",
        "    save_path = \"charRNN_shakespeare.pth\"\n",
        "    data_path = \"input.txt\"\n",
        "    #######################################\n",
        "\n",
        "    # load the text file\n",
        "    data = open(data_path, 'r').read()\n",
        "    chars = sorted(list(set(data)))\n",
        "    data_size, vocab_size = len(data), len(chars)\n",
        "    print(\"----------------------------------------\")\n",
        "    print(\"Data has {} characters, {} unique\".format(data_size, vocab_size))\n",
        "    print(\"----------------------------------------\")\n",
        "\n",
        "    # char to index and index to char maps\n",
        "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "    ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "    # convert data from chars to indices\n",
        "    data = list(data)\n",
        "    for i, ch in enumerate(data):\n",
        "        data[i] = char_to_ix[ch]\n",
        "\n",
        "    # data tensor on device\n",
        "    data = torch.tensor(data).to(device)\n",
        "    data = torch.unsqueeze(data, dim=1)\n",
        "\n",
        "    # model instance\n",
        "    rnn = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "    # load checkpoint if True\n",
        "    if load_chk:\n",
        "        rnn.load_state_dict(torch.load(save_path))\n",
        "        print(\"Model loaded successfully !!\")\n",
        "        print(\"----------------------------------------\")\n",
        "\n",
        "    # loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
        "\n",
        "    # training loop\n",
        "    for i_epoch in range(1, epochs+1):\n",
        "\n",
        "        # random starting point (1st 100 chars) from data to begin\n",
        "        data_ptr = np.random.randint(100)\n",
        "        n = 0\n",
        "        running_loss = 0\n",
        "        hidden_state = None\n",
        "\n",
        "        while True:\n",
        "            input_seq = data[data_ptr : data_ptr+seq_len]\n",
        "            target_seq = data[data_ptr+1 : data_ptr+seq_len+1]\n",
        "\n",
        "            # forward pass\n",
        "            output, hidden_state = rnn(input_seq, hidden_state)\n",
        "\n",
        "            # compute loss\n",
        "            loss = loss_fn(torch.squeeze(output), torch.squeeze(target_seq))\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # compute gradients and take optimizer step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update the data pointer\n",
        "            data_ptr += seq_len\n",
        "            n +=1\n",
        "\n",
        "            # if at end of data : break\n",
        "            if data_ptr + seq_len + 1 > data_size:\n",
        "                break\n",
        "\n",
        "        # print loss and save weights after every epoch\n",
        "        print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(i_epoch, running_loss/n))\n",
        "        torch.save(rnn.state_dict(), save_path)\n",
        "\n",
        "        # sample / generate a text sequence after every epoch\n",
        "        data_ptr = 0\n",
        "        hidden_state = None\n",
        "\n",
        "        # random character from data to begin\n",
        "        rand_index = np.random.randint(data_size-1)\n",
        "        input_seq = data[rand_index : rand_index+1]\n",
        "\n",
        "        print(\"----------------------------------------\")\n",
        "        while True:\n",
        "            # forward pass\n",
        "            output, hidden_state = rnn(input_seq, hidden_state)\n",
        "\n",
        "            # construct categorical distribution and sample a character\n",
        "            output = F.softmax(torch.squeeze(output), dim=0)\n",
        "            dist = Categorical(output)\n",
        "            index = dist.sample()\n",
        "\n",
        "            # print the sampled character\n",
        "            print(ix_to_char[index.item()], end='')\n",
        "\n",
        "            # next input is current output\n",
        "            input_seq[0][0] = index.item()\n",
        "            data_ptr += 1\n",
        "\n",
        "            if data_ptr > op_seq_len:\n",
        "                break\n",
        "\n",
        "        print(\"\\n----------------------------------------\")\n",
        "\n",
        "train()\n",
        "\n"
      ],
      "metadata": {
        "id": "4NTUPgz9sTMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BiRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size):\n",
        "        super(BiRNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.birnn = nn.RNN(embedding_dim, hidden_dim, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        birnn_out, _ = self.birnn(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(birnn_out.view(len(sentence), -1))\n",
        "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n"
      ],
      "metadata": {
        "id": "NKgkcOHctmrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [([\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"DET\", \"NOUN\", \"VERB\", \"ADP\", \"DET\", \"NOUN\"]),\n",
        "                 ([\"The\", \"dog\", \"ate\", \"my\", \"homework\"], [\"DET\", \"NOUN\", \"VERB\", \"ADJ\", \"NOUN\"])]\n"
      ],
      "metadata": {
        "id": "8sLssj4sRfdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the training data\n",
        "train_data = [(\"The\", \"DET\"), (\"dog\", \"NOUN\"), (\"chased\", \"VERB\"), (\"the\", \"DET\"), (\"cat\", \"NOUN\")]\n",
        "\n",
        "# Define the vocabulary\n",
        "vocab = set(word for word, tag in train_data)\n",
        "\n",
        "# Define the tagset\n",
        "tagset = set(tag for word, tag in train_data)\n",
        "\n",
        "# Define the mapping between words and indices\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "# Define the mapping between tags and indices\n",
        "tag_to_idx = {tag: i for i, tag in enumerate(tagset)}\n",
        "\n",
        "# Define the PyTorch model\n",
        "class SimpleTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim):\n",
        "        super(SimpleTagger, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "\n",
        "# Instantiate the model\n",
        "EMBEDDING_DIM = 10\n",
        "HIDDEN_DIM = 10\n",
        "model = SimpleTagger(len(vocab), len(tagset), EMBEDDING_DIM, HIDDEN_DIM)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(100):\n",
        "    for sentence, tags in train_data:\n",
        "        # Convert the sentence and tags to PyTorch tensors\n",
        "        sentence_in = torch.tensor([word_to_idx[word] for word in sentence], dtype=torch.long)\n",
        "        targets = torch.tensor([tag_to_idx[tag] for tag in tags], dtype=torch.long)\n",
        "\n",
        "        # Clear out the gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Run the forward pass\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        # Compute the loss, gradients, and update the parameters\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Test the model on a new sentence\n",
        "test_sentence = \"The cat sat on the mat\".split()\n",
        "test_sentence_in = torch.tensor([word_to_idx[word] for word in test_sentence], dtype=torch.long)\n",
        "tag_scores = model(test_sentence_in)\n",
        "_, predicted_tags = torch.max(tag_scores, dim=1)\n",
        "predicted_tags = [list(tag_to_idx.keys())[list(tag_to_idx.values()).index(idx)] for idx in predicted_tags]\n",
        "print(test_sentence)\n",
        "print(predicted_tags)\n"
      ],
      "metadata": {
        "id": "Qr38gXgCR3fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the LSTM-based POS tagger model\n",
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "\n",
        "# Define the training function\n",
        "def train(model, optimizer, loss_function, sentences, tags, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        for sentence, tag in zip(sentences, tags):\n",
        "            model.zero_grad()\n",
        "            sentence = torch.tensor(sentence, dtype=torch.long)\n",
        "            tag = torch.tensor(tag, dtype=torch.long)\n",
        "            tag_scores = model(sentence)\n",
        "            loss = loss_function(tag_scores, tag)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "# Define the evaluation function\n",
        "def evaluate(model, sentences, tags):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for sentence, tag in zip(sentences, tags):\n",
        "            sentence = torch.tensor(sentence, dtype=torch.long)\n",
        "            tag = torch.tensor(tag, dtype=torch.long)\n",
        "            tag_scores = model(sentence)\n",
        "            _, predicted = torch.max(tag_scores.data, 1)\n",
        "            total += tag.size(0)\n",
        "            correct += (predicted == tag).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Example usage\n",
        "# Define a sample corpus and its corresponding tags\n",
        "corpus = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog chased the cat\",\n",
        "    \"The mouse ran away from the cat\",\n",
        "    \"The cat purred\",\n",
        "]\n",
        "tags = [\n",
        "    \"DET NOUN VERB ADP DET NOUN\",\n",
        "    \"DET NOUN VERB DET NOUN\",\n",
        "    \"DET NOUN VERB ADV ADP DET NOUN\",\n",
        "    \"DET NOUN VERB\",\n",
        "]\n",
        "# Define the vocabulary and POS tagset\n",
        "word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1, \"The\": 2, \"cat\": 3, \"sat\": 4, \"on\": 5, \"the\": 6,\n",
        "              \"mat\": 7, \"dog\": 8, \"chased\": 9, \"mouse\": 10, \"ran\": 11, \"away\": 12, \"from\": 13, \"purred\": 14}\n",
        "tag_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1, \"DET\": 2, \"NOUN\": 3, \"VERB\": 4, \"ADP\": 5, \"ADV\": 6}\n",
        "\n",
        "# Convert the corpus and tagset to indices\n",
        "sentences = [[word_to_ix.get(word, word_to_ix[\"<UNK>\"]) for word in sentence.split()] for sentence in corpus]\n",
        "tags = [[tag_to_ix.get(tag, tag_to_ix[\"<UNK>\"]) for tag in sentence.split()] for sentence in tags]\n",
        "\n",
        "# Set hyperparameters and create model, optimizer, and loss function instances\n",
        "vocab_size = len(word_to_ix)\n",
        "embedding_dim = 16\n",
        "hidden_dim = 16\n",
        "tagset_size = len(tag_to_ix)\n",
        "num_epochs = 10\n",
        "learning_rate = 0.1\n",
        "model = LSTMTagger(vocab_size, embedding_dim, hidden_dim, tagset_size)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.NLLLoss()\n",
        "#Train the model\n",
        "train(model, optimizer, loss_function, sentences, tags, num_epochs)\n",
        "\n",
        "#Evaluate the model on the same corpus\n",
        "accuracy = evaluate(model, sentences, tags)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "k0RovHYIR58R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kaWXDwCzjw6c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}